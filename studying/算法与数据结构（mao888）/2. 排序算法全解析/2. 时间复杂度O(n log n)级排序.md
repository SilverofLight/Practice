# 希尔排序

## 原理和实现

1959 年 7 月，美国辛辛那提大学的数学系博士 `Donald Shell` 在 《ACM 通讯》上发表了希尔排序算法，成为首批将时间复杂度降到 $O(n^2)$ 以下的算法之一。虽然原始的希尔排序最坏时间复杂度仍然是 $O(n^2)$ ，但经过优化的希尔排序可以达到 $O(n^{1.3})$ 甚至 $O(n^{7/6})$。

略为遗憾的是，所谓「一将功成万骨枯」，希尔排序和冒泡、选择、插入等排序算法一样，逐渐被快速排序所淘汰，但作为承上启下的算法，不可否认的是，希尔排序身上始终闪耀着算法之美。

它的基本思想是：

- 将待排序数组按照一定的间隔分为多个子数组，每组分别进行插入排序。这里按照间隔分组指的不是取连续的一段数组，而是每跳跃一定间隔取一个值组成一组
- 逐渐缩小间隔进行下一轮排序
- 最后一轮时，取间隔为 1，也就相当于直接使用插入排序。但这时经过前面的「宏观调控」，数组已经基本有序了，所以此时的插入排序只需进行少量交换便可完成

举个例子，对数组 [84, 83, 88, 87, 61, 50, 70, 60, 80, 99] 进行希尔排序的过程如下：

- 第一遍（5 间隔排序）：按照间隔 5 分割子数组，共分成五组，分别是 [84,50],[83,70],[88,60],[87,80],[61,99]。对它们进行插入排序，排序后它们分别变成： [50, 84], [70, 83], [60, 88], [80, 87], [61, 99]，此时整个数组变成 [50,70,60,80,61,84,83,88,87,99]
- 第二遍（2 间隔排序）：按照间隔 2分割子数组，共分成两组，分别是 [50, 60, 61, 83, 87], [70, 80, 84, 88, 99]。对他们进行插入排序，排序后它们分别变成： [50, 60, 61, 83, 87], [70, 80, 84, 88, 99]，此时整个数组变成 [50,70,60,80,61,84,83,88,87,99]。这里有一个非常重要的性质：当我们完成 2间隔排序后，这个数组仍然是保持 5 间隔有序的。也就是说，**更小间隔的排序没有把上一步的结果变坏**。
- 第三遍（1 间隔排序，等于直接插入排序）：按照间隔 1 分割子数组，分成一组，也就是整个数组。对其进行插入排序，经过前两遍排序，数组已经基本有序了，所以这一步只需经过少量交换即可完成排序。排序后数组变成 [50,60,61,70,80,83,84,87,88,99]，整个排序完成。

其中，每一遍排序的间隔在希尔排序中被称之为增量，所有的增量组成的序列称之为增量序列，也就是本例中的 [5, 2, 1]。增量依次递减，最后一个增量必须为 1，所以希尔排序又被称之为「缩小增量排序」。要是以专业术语来描述希尔排序，可以分为以下两个步骤：

- 定义增量序列 $D_m>D_{m−1}>D_{m−2}>...>D_1=1$
- 对每个 $D_k$ 进行 「$D_k$ 间隔排序」

有一条非常重要的性质保证了希尔排序的效率：

- 「$D_{k+1}$ 间隔」 有序的序列，在经过 「$D_k$ 间隔」 排序后，仍然是 「$D_{k+1}$ 间隔」 有序的

增量序列的选择会极大地影响希尔排序的效率。本例中，我们采用的增量序列为 $D_m = N/2, D_k = D_{k+1} / 2$，这个序列正是当年希尔发表此算法的论文时选用的序列，所以也被称之为**希尔增量序列**。代码实现如下：

```go
func shellSort(arr []int) {
	//间隔序列
	for gap := len(arr)/2; gap > 0; gap /= 2 {
		//分组
		for groupStartIndex := 0; groupStartIndex < gap; groupStartIndex++ {
			//插入排序
			for currentIndex = groupStartIndex + gap; currentIndex < len(arr); currentIndex += gap {
				//currentNumber 站起来，开始找位置
				currentNumber := arr[currentIndex]
				preIndex := currentIndex - gap
				for preIndex >= groupStartIndex && currentNumber < arr[preIndex] {
					//向后挪位置
					arr[preIndex + gap] = arr[preIndex]
					preIndex -= gap
				}
				//currentNumber 找到位置，左下
				arr[preIndex + gap] = currentNmber
			}
		} 
	}
}
```

实际上，这段代码可以优化一下。我们现在的处理方式是：处理完一组间隔序列后，再回来处理下一组间隔序列，这非常符合人类思维。但对于计算机来说，它更喜欢从第 `gap` 个元素开始，按照顺序将每个元素依次向前插入自己所在的组这种方式。虽然这个过程看起来是在不同的间隔序列中不断跳跃，但站在计算机的角度，它是在访问一段连续数组。

```go
func shellSort(arr []int) {
	for gap := len(arr)/2; gap > 0; gap /= 2 {
		//从gap开始，按照顺序将每个元素依次向前插入自己所在的组
		for i := gap; i < len(arr); i++ {
			//currentNumber找位置
			currentNumber := arr[i]
			//该组前一个数字的索引
			preIndex = i - gap
			for preIndex >= 0 && currentNumber < arr[preIndex] {
				arr[preIndex + gap] = arr[preIndex]
				preIndex -= gap
			}
			arr[preIndex + gap] = currentNmber
		}
	}
}
```

经过优化之后，这段代码看起来就和插入排序非常相似了，区别仅在于希尔排序最外层嵌套了一个缩小增量的 for 循环；并且插入时不再是相邻数字挪动，而是以增量为步长挪动。

## 增量序列

上文说到，增量序列的选择会极大地影响希尔排序的效率。增量序列如果选得不好，希尔排序的效率可能比插入排序效率还要低，举个例子：

![img](https://camo.githubusercontent.com/3de75b673637272f11e27ed9bd887702cdfe80338a5faf4aac550ffa35c5c72c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032322f706e672f32323231393438332f313636383335343938323735322d38333839373062622d353264392d343862332d386632622d6661323038636534633363332e706e6723617665726167654875653d25323364656463646226636c69656e7449643d7565323362643163322d623830382d342663726f703d302663726f703d302663726f703d312663726f703d312669643d6b43686278266e616d653d696d6167652e706e67266f726967696e4865696768743d353134266f726967696e57696474683d31303836266f726967696e616c547970653d62696e61727926726174696f3d3126726f746174696f6e3d302673686f775469746c653d66616c73652673697a653d313331313335267374617475733d646f6e65267374796c653d6e6f6e65267461736b49643d7563623635656662362d313931362d343963642d626163332d3430343362643162313962267469746c653d)

在这个例子中，我们发现，原数组 `8` 间隔、`4` 间隔、`2` 间隔都已经有序了，使用希尔排序时，真正起作用的只有最后一轮 `1` 间隔排序，也就是直接插入排序。希尔排序反而比直接使用插入排序多执行了许多无用的逻辑。

于是人们发现：增量元素不互质，则小增量可能根本不起作用.

事实上，希尔排序的增量序列如何选择是一个数学界的难题，但它也是希尔排序算法的核心优化点。数学界有不少的大牛做过这方面的研究。比较著名的有 `Hibbard` 增量序列、`Knuth` 增量序列、`Sedgewick` 增量序列。

- `Hibbard` 增量序列：$D_k = 2^k - 1$，也就是 1,3,7,15,...。数学界猜想它最坏的时间复杂度为 $O(n^{3/2})$，平均时间复杂度为 $O(n^{5/4})$ ;
- `Knuth` 增量序列：$D_1 = 1; D_{k+1} = 3 * D_k + 1$，也就是 1,4,13,40,...，数学界猜想它的平均时间复杂度为 $O(n^{3/2})$；
- `Sedgewick` 增量序列：1,5,19,41,109,...，这个序列的元素有的是通过 $9 * 4^k - 9 * 2^k + 1$ 计算出来的，有的是通过 $4^k - 3 * 2^k + 1$ 计算出来的。数学界猜想它最坏的时间复杂度为 $O(n^{4/3})$，平均时间复杂度为 $O(n^{7/6})$.

## 时间复杂度和空间复杂度

事实上，希尔排序时间复杂度非常难以分析，它的平均复杂度界于 O(n) 到 O(n^2) 之间，普遍认为它最好的时间复杂度为 O(n^{1.3})。

希尔排序的空间复杂度为 O(1)，只需要常数级的临时变量。

## 希尔排序与O(n^2)级算法的本质区别

这个问题我们可以用逆序对来理解。

> 当我们从小到大排序时，在数组中的两个数字，如果前面一个数字大于后面的数字，则这两个数字组成一个逆序对。

排序算法本质上就是一个消除逆序对的过程。

对于随机数组，逆序对的数量是 O(n^2) 级的，如果采用「交换相邻元素」的办法来消除逆序对，每次最多只能消除一组逆序对，因此必须执行 O(n^2) 级的交换次数，这就是为什么冒泡、插入、选择算法只能到O(n^2) 级的原因。

反过来说，基于交换元素的排序算法要想突破 O(n^2) 级，必须通过一些比较，交换间隔比较远的元素，使得一次交换能消除一个以上的逆序对。

希尔排序算法就是通过这种方式，打破了在空间复杂度为O(1) 的情况下，时间复杂度为 O(n^2) 的魔咒，此后的快排、堆排等等算法也都是基于这样的思路实现的。

> 注： 1.虽然约翰·冯·诺伊曼在 1945 年提出的归并排序已经达到了 O(nlog⁡n)的时间复杂度，但归并排序的空间复杂度为 O(n)，采用的是空间换时间的方式突破 O(n^2) 。
> 
> 2.希尔排序在面试或是实际应用中都很少遇到，读者仅需了解即可。

# 堆排序

## 思路

> 堆：符合以下两个条件之一的完全二叉树：
> 
> - 根节点的值 ≥ 子节点的值，这样的堆被称之为最大堆，或大顶堆；
> - 根节点的值 ≤ 子节点的值，这样的堆被称之为最小堆，或小顶堆。

为了有一个轻松的开场，我们先来看一个程序员的段子放松一下：

> 你有哪些用计算机技能解决生活问题的经历？
> 
> 我认识一个大牛，他不喜欢洗袜子，又不喜欢袜子的臭味。于是他买了很多样式一样的袜子，把这些袜子放在地上，根据臭的程度，摆一个二叉堆。每天早上，他 `pop` 两只最“香”的袜子，穿上；晚上回到家，把袜子脱下来，`push` 到堆里。某一天，`top` 的袜子超过他的耐臭能力，全扔掉，买新的。

如果我们将袜子 「臭的程度」 量化，这位大牛每天做的事情就是构建一个大顶堆，然后将堆顶的袜子取出来。再调整剩下的袜子，构建出一个新的大顶堆，再次取出堆顶的袜子。这个过程使用的就是堆排序的思想，它是由 `J. W. J. Williams` 在 1964 年发明的。

堆排序过程如下：

- 用数列构建出一个大顶堆，取出堆顶的数字；
- 调整剩余的数字，构建出新的大顶堆，再次取出堆顶的数字；
- 循环往复，完成整个排序。

整体的思路就是这么简单，我们需要解决的问题有两个：

- 如何用数列构建出一个大顶堆；
- 取出堆顶的数字后，如何将剩余的数字调整成新的大顶堆。

## 构建大顶堆&调整堆

构建大顶堆有两种方式：

- 方案一：从 `0` 开始，将每个数字依次插入堆中，一边插入，一边调整堆的结构，使其满足大顶堆的要求；
- 方案二：将整个数列的初始状态视作一棵完全二叉树，自底向上调整树的结构，使其满足大顶堆的要求。

方案二更常用

在介绍堆排序具体实现之前，我们先要了解完全二叉树的几个性质。将根节点的下标视为 `0`，则完全二叉树有如下性质：

- 对于完全二叉树中的第 `i` 个数，它的左子节点下标：`left = 2i + 1`
- 对于完全二叉树中的第 `i` 个数，它的右子节点下标：`right = left + 1`
- 对于有 `n` 个元素的完全二叉树(n≥2)，它的最后一个非叶子结点的下标：`n/2 - 1`

堆排序代码如下：
```go
func heapSort(arr []int){
	//构建初始大顶堆
	buildMaxHeap(arr)
	for i := len(arr)-1; i > 0; i-- {
		//将最大值交换到数组最后
		arr[0], arr[i] = arr[i], arr[0]
		//调整剩余数组，使其满足大顶堆
		maxHeapify(arr, 0, i)
	}
}

//初始化
func buildMaxHeap(arr []int) {
	//从最后一个非叶子节点开始调整大顶堆，最后一个非叶子节点的下标就是len(arr)/2 - 1
	for i := len(arr)/2 - 1; i >= 0; i-- {
		maxHeapify(arr, i, len(arr))
	}
}

// 调整大顶堆，第三个参数表示剩余未排序的数字的数量，也就是剩余堆的大小
func maxHeapify(arr []int, i int, heapSize int) {
	//左下节点下标
	l := 2 * i + 1
	// 右下节点下标
	r := l + 1
	//记录根节点、左子树节点、右子树节点三者中的最大值下标
	largest := i
	if l < heapSize && arr[l] > arr[largest] {
		largest = l
	}
	if r < heapSize && arr[r] > arr[largest] {
		largest = r
	}
	if largest != i {
		//将最大值交换为根节点
		arr[i], arr[largest] = arr[largest], arr[i]
		//再次调整交换后的大顶堆
		maxHeapify(arr, largest, heapSize)
	}
}
```
堆排序的第一步就是构建大顶堆，对应代码中的 `buildMaxHeap` 函数。我们将数组视作一颗完全二叉树，从它的最后一个非叶子结点开始，调整此结点和其左右子树，使这三个数字构成一个大顶堆。

调整过程由 `maxHeapify` 函数处理， `maxHeapify` 函数记录了最大值的下标，根结点和其左右子树结点在经过比较之后，将最大值交换到根结点位置。这样，这三个数字就构成了一个大顶堆。

需要注意的是，如果根结点和左右子树结点任何一个数字发生了交换，则还需要保证调整后的子树仍然是大顶堆，所以子树会执行一个递归的调整过程。

当构建出大顶堆之后，就要把冠军交换到数列最后，深藏功与名。来到冠军宝座的新人又要和李小胖一样，开始向下比较，找到自己的真实位置，使得剩下的 n - 1 个数字构建成新的大顶堆。这就是 `heapSort` 方法的 `for` 循环中，调用 `maxHeapify` 的原因。

变量 `heapSize` 用来记录还剩下多少个数字没有排序完成，每当交换了一个堆顶的数字，`heapSize` 就会减 1。在 `maxHeapify` 方法中，使用 `heapSize` 来限制剩下的选手，不要和已经躺在数组最后，当过冠军的人比较，免得被暴揍。

> 注：在有的文章中，作者将堆的根节点下标视为 11，这样做的好处是使得第 `i` 个结点的左子结点下标为 `2i`，右子结点下标为 `2i + 1`，与 `2i + 1` 和 `2i + 2` 相比，计算量会少一点，本文未采取这种实现，但两种实现思路的核心思想都是一致的。

## 时间复杂度 & 空间复杂度

堆排序分为两个阶段：初始化建堆（`buildMaxHeap`）和重建堆（`maxHeapify`，直译为大顶堆化）。所以时间复杂度要从这两个方面分析。

根据数学运算可以推导出初始化建堆的时间复杂度为 O(n)，重建堆的时间复杂度为 O(nlog⁡n)，所以堆排序总的时间复杂度为 O(nlog⁡n)。推导过程较为复杂，故不再给出证明过程。

堆排序的空间复杂度为 O(1)，只需要常数级的临时变量。


# 快速排列

快速排序算法由 C. A. R. Hoare 在 1960 年提出。它的时间复杂度也是 O(nlogn)，但它在时间复杂度为 O(nlogn) 级的几种排序算法中，大多数情况下效率更高，所以快速排序的应用非常广泛。再加上快速排序所采用的分治思想非常实用，使得快速排序深受面试官的青睐，所以掌握快速排序的思想尤为重要。

快速排序算法的基本思想是：

- 从数组中取出一个数，称之为基数（pivot）
- 遍历数组，将比基数大的数字放到它的右边，比基数小的数字放到它的左边。遍历完成后，数组被分成了左右两个区域
- 将左右两个区域视为两个数组，重复前两个步骤，直到排序完成

事实上，快速排序的每一次遍历，都将基数摆到了最终位置上。第一轮遍历排好 1 个基数，第二轮遍历排好 2 个基数（每个区域一个基数，但如果某个区域为空，则此轮只能排好一个基数），第三轮遍历排好 4 个基数（同理，最差的情况下，只能排好一个基数），以此类推。总遍历次数为 logn～n 次，每轮遍历的时间复杂度为 O(n)，所以很容易分析出快速排序的时间复杂度为 O(nlogn) ～ O(n^2)，平均时间复杂度为 O(nlogn)。

## 快速排序递归框架

```go
func quickSort(arr []int) {
	quickSort2(arr, 0, len(arr)-1)
}

func quickSort2(arr []int, start int, end int){
	//将数组分区，并获得中间值的下标
	middle := partition(arr, start, end)
	//对左边区域快排
	quickSort2(arr, start, middle - 1)
	//对右边区域快排
	quickSort2(arr, middle + 1, end)
}

func partition(arr []int, start int, end int){
	//将arr从start到end分区，左边区域比基数小，右边区域比基数大，然后返回中间值的下标
}
```
partition 意为“划分”，我们期望 partition 函数做的事情是：将 arr 从 start 到 end 这一区间的值分成两个区域，左边区域的每个数都比基数小，右边区域的每个数都比基数大，然后返回中间值的下标。

只要有了这个函数，我们就能写出快速排序的递归函数框架。首先调用 partition 函数得到中间值的下标 middle，然后对左边区域执行快速排序，也就是递归调用 `quickSort(arr, start, middle - 1)`，再对右边区域执行快速排序，也就是递归调用 `quickSort(arr, middle + 1, end)`。

现在还有一个问题，何时退出这个递归函数呢？

## 退出递归的边界条件

很容易想到，当某个区域只剩下一个数字的时候，自然不需要排序了，此时退出递归函数。实际上还有一种情况，就是某个区域只剩下 0 个数字时，也需要退出递归函数。当 middle 等于 start 或者 end 时，就会出现某个区域剩余数字为 0。

所以我们可以通过这种方式退出递归函数：

~~~go
func quickSort2(arr []int, start int, end int){
	//将数组分区，并获得中间值的下标
	middle := partition(arr, start, end)
	// 当左边区域中至少有 2 个数字时，对左边区域快排
	if start != middle && start != middle - 1 {
		quickSort2(arr, start, middle - 1)
	}
	// 当右边区域中至少有 2 个数字时，对右边区域快排
	if middle != end && middle != end - 1 {
		quickSort2(arr, middle + 1, end)
	}
}
~~~

在递归之前，先判断此区域剩余数字是否为 0 个或者 1 个，当数字至少为 2 个时，才执行这个区域的快速排序。因为我们知道 `middle >= start && middle <= end` 必然成立，所以判断剩余区域的数字为 0 个或者 1 个也就是指 start 或 end 与 middle 相等或相差 1。

我们来分析一下这四个判断条件：

- 当 `start == middle` 时，相当于 `quickSort(arr, start, middle - 1)` 中的 start == end + 1
- 当 `start == middle - 1` 时，相当于 `quickSort(arr, start, middle - 1)` 中的 start == end
- 当 `middle == end` 时，相当于 `quickSort(arr, middle + 1, end)` 中的 start == end + 1
- 当 `middle == end -1` 时，相当于 `quickSort(arr, middle + 1, end)` 中的 start == end

综上，我们可以将此边界条件统一移到 quickSort 函数之前:

~~~go
func quickSort2(arr []int, start int, end int){
	// 如果区域中的数字少于2个，退出递归
	if start == end || start == end + 1 {
		return
	}
	//将数组分区，并获得中间值的下标
	middle := partition(arr, start, end)
	//对左边区域快排
	quickSort2(arr, start, middle - 1)
	//对右边区域快排
	quickSort2(arr, middle + 1, end)
}
~~~

更进一步，由上文所说的 `middle >= start && middle <= end` 可以推出，除了`start == end || start == end + 1`这两个条件之外，其他的情况下 start 都小于 end。所以我们可以将这个判断条件再次简写为：

~~~go
func quickSort2(arr []int, start int, end int){
	// 如果区域中的数字少于2个，退出递归
	if start >= end {
		return
	}
	//将数组分区，并获得中间值的下标
	middle := partition(arr, start, end)
	//对左边区域快排
	quickSort2(arr, start, middle - 1)
	//对右边区域快排
	quickSort2(arr, middle + 1, end)
}
~~~

## 分区算法实现

快速排序中最重要的便是分区算法，也就是 partition 函数。大多数人都能说出快速排序的整体思路，但实现起来却很难一次写对。主要问题就在于分区时存在的各种边界条件，需要读者亲自动手实践才能加深体会。

上文已经说到，partition 函数需要做的事情就是将 arr 从 start 到 end 分区，左边区域比基数小，右边区域比基数大，然后返回中间值的下标。那么首先我们要做的事情就是选择一个基数，基数我们一般称之为 pivot，意为“轴”。整个数组就像围绕这个轴进行旋转，小于轴的数字旋转到左边，大于轴的数字旋转到右边。（所谓的双轴快排就是一次选取两个基数，将数组分为三个区域进行旋转，关于双轴快排的内容我们将在后续章节讲解。）

### 基数的选择

基数的选择没有固定标准，随意选择区间内任何一个数字做基数都可以。通常来讲有三种选择方式：

- 选择第一个元素作为基数
- 选择最后一个元素作为基数
- 选择区间内一个随机元素作为基数

选择的基数不同，算法的实现也不同。实际上第三种选择方式的平均时间复杂度是最优的，待会分析时间复杂度时我们会详细说明。

本文通过第一种方式来讲解快速排序：

~~~go
func partition(arr []int, start int, end int){
	// 取第一个数为基数
	pivot := arr[start]
	// 取第二个数开始分区
	left := start + 1
	// 右边界
	right := end
	// TODO
}
~~~
### 最简单的分区算法

分区的方式也有很多种，最简单的思路是：从 left 开始，遇到比基数大的数，就交换到数组最后，并将 right 减一，直到 left 和 right 相遇，此时数组就被分成了左右两个区域。再将基数和中间的数交换，返回中间值的下标即可。

按照这个思路，我们敲出了如下代码：

~~~go
func partition(arr []int, start ing, end int) {
	// 取第一个数为基数
	pivot := arr[start]
	// 取第二个数开始分区
	left := start + 1
	// 右边界
	right := end
	// left right相遇时提出循环
	for left < right {
		//找到第一个大于基数的位置
		for left < right && arr[left] <= pivot {
			left++
		}
		// 交换
		if left != right {
			arr[left], arr[right] = arr[right], arr[left]
			right--
		}
	}
	//如果左右相等，单独比较arr[right]和pivot
	if left == right && arr[right] > pivot {
		right--
	}
	// 将基数和中间数互换
	if right != start {
		arr[start], arr[right] = arr[right], arr[start]
	}
	// 返回中间值的下标
	return right
}
~~~

因为我们选择了数组的第一个元素作为基数，并且分完区后，会执行将基数和中间值交换的操作，这就意味着交换后的中间值会被分到左边区域。所以我们需要保证中间值的下标是分区完成后，最后一个比基数小的值，这里我们用 right 来记录这个值。

这段代码有一个细节。首先，在交换 left 和 right 之前，我们判断了 left != right，这是因为如果剩余的数组都比基数小，则 left 会加到 right 才停止，这时不应该发生交换。因为 right 已经指向了最后一个比基数小的值。

但这里的拦截可能会拦截到一种错误情况，如果剩余的数组只有最后一个数比基数大，left 仍然加到 right 停止了，但我们并没有发生交换。所以我们在退出循环后，单独比较了 arr[right] 和 pivot。

实际上，这行单独比较的代码非常巧妙，一共处理了三种情况：

- 一是刚才提到的剩余数组中只有最后一个数比基数大的情况
- 二是 left 和 right 区间内只有一个值，则初始状态下， left == right，所以 `while (left < right)` 根本不会进入，所以此时我们单独比较这个值和基数的大小关系
- 三是剩余数组中每个数都比基数大，此时 right 会持续减小，直到和 left 相等退出循环，此时 left 所在位置的值还没有和 pivot 进行比较，所以我们单独比较 left 所在位置的值和基数的大小关系

### 双指针分区算法

除了上述的分区算法外，还有一种双指针的分区算法更为常用：从 left 开始，遇到比基数大的数，记录其下标；再从 right 往前遍历，找到第一个比基数小的数，记录其下标；然后交换这两个数。继续遍历，直到 left 和 right 相遇。然后就和刚才的算法一样了，交换基数和中间值，并返回中间值的下标。

代码如下：

~~~go

~~~